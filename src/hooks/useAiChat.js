import { invoke } from '@tauri-apps/api/core';
import { useOptionStore } from '../store/option';

const useAiChat = () => {
    const sendRequest = (url, method, headers, body, proxy) => {
        return new Promise((resolve, reject) => {

            proxy = proxy ? proxy : null;
            invoke('send_api_request', {
                request: {
                    url,
                    method,
                    headers,
                    body,
                    proxy,
                },
            }).then((response)=>{
                if (response.code == 200) {
                    resolve(response);
                } else {
                    reject(response.msg || 'æœªçŸ¥é”™è¯¯')
                }
            }).catch((error)=>{
                reject(error);
            });
        })
    };

    /**
     * ChatGLM æ¨¡å‹
     * @param {string} prompt ç”¨æˆ·è¾“å…¥
     * @param {string} model æ¨¡å‹åç§° (å¦‚ "glm-4", "glm-4-plus")
     * @param {string} apiKey API å¯†é’¥
     * @param {string} [proxy] å¯é€‰çš„ä»£ç†åœ°å€ (å¦‚ "http://user:pass@host:port" æˆ– "socks5://user:pass@host:port")
     * @returns {Promise<{success: boolean, data?: string, error?: string}>} AI æ¨¡å‹è¾“å‡ºæˆ–é”™è¯¯ä¿¡æ¯
     */
    const chatGLM = (prompt, model, apiKey, proxy) => {
        return new Promise((resolve, reject) => {
            const url = 'https://open.bigmodel.cn/api/paas/v4/chat/completions';
            const headers = {
                'Authorization': `Bearer ${apiKey}`,
                'Content-Type': 'application/json',
            };
            const body = {
                model: model,
                messages: [
                    {
                        role: 'user',
                        content: prompt,
                    },
                ],
                stream: false, // ç¦ç”¨æµå¼ä¼ è¾“
            };

            sendRequest(url, 'POST', headers, body, proxy).then((result)=>{
                const data = result.data.data;

                const choices = data?.choices;
                if (choices && choices.length > 0) {
                    const message = choices[0].message;
                    if (message.content) {
                        resolve(message.content);
                        return;
                    }
                }

                reject('ChatGLM å“åº”æ ¼å¼ä¸æ­£ç¡®æˆ–æ— å†…å®¹');
            }).catch((error)=>{
                // æ¥å£é”™è¯¯è¿”å›ç¤ºä¾‹ï¼š"API è¯·æ±‚å¤±è´¥ï¼ŒçŠ¶æ€ç : 401 Unauthorizedï¼Œå“åº”: {\"error\":{\"code\":\"401\",\"message\":\"ä»¤ç‰Œå·²è¿‡æœŸæˆ–éªŒè¯ä¸æ­£ç¡®ï¼\"}}"
                // æ‡’å¾—è§£æäº†ï¼Œç›´æ¥å±•ç¤ºç»™ç”¨æˆ·å§ğŸ˜€
                reject(error);
            });
        })
        
    };

    /**
     * DeepSeek æ¨¡å‹ *ä½œè€…æ²¡æœ‰deepSeek Api è¯¥å‡½æ•°æœªç»æµ‹è¯•ï¼*
     * @param {string} prompt ç”¨æˆ·è¾“å…¥
     * @param {string} model æ¨¡å‹åç§° (å¦‚ "deepseek-chat", "deepseek-reasoner")
     * @param {string} apiKey API å¯†é’¥
     * @param {string} [proxy] å¯é€‰çš„ä»£ç†åœ°å€
     * @returns {Promise<{success: boolean, data?: string, error?: string}>} AI æ¨¡å‹è¾“å‡ºæˆ–é”™è¯¯ä¿¡æ¯
     */
    const deepSeek = (prompt, model, apiKey, proxy) => {
        return new Promise((resolve, reject) => {
            const url = 'https://api.deepseek.com/chat/completions';
            const headers = {
                'Content-Type': 'application/json',
                'Accept': 'application/json',
                'Authorization': `Bearer ${apiKey}`,
            };
            const body = {
                messages: [
                    {
                        content: 'You are a helpful assistant',
                        role: 'system',
                    },
                    {
                        content: prompt,
                        role: 'user',
                    },
                ],
                model: model,
                stream: false,
            };

            sendRequest(url, 'POST', headers, body, proxy).then((result)=>{
                const data = result.data.data;
                const choices = data?.choices;
                if (choices && choices.length > 0 && choices[0].message && choices[0].message.content) {
                    resolve(choices[0].message.content);
                }else{
                    reject('DeepSeek å“åº”æ ¼å¼ä¸æ­£ç¡®æˆ–æ— å†…å®¹');
                }
            }).catch((error)=>{
                reject(error);
            })
        })
    };

    /**
     * Groq æ¨¡å‹
     * @param {string} prompt ç”¨æˆ·è¾“å…¥
     * @param {string} model æ¨¡å‹åç§° (å¦‚ "llama3-8b-8192", "llama3-70b-8192")
     * @param {string} apiKey API å¯†é’¥
     * @param {string} [proxy] å¯é€‰çš„ä»£ç†åœ°å€
     * @returns {Promise<{success: boolean, data?: string, error?: string}>} AI æ¨¡å‹è¾“å‡ºæˆ–é”™è¯¯ä¿¡æ¯
     */
    const groq = (prompt, model, apiKey, proxy) => {
        return new Promise((resolve, reject) => {
            const url = 'https://api.groq.com/openai/v1/chat/completions';
            const headers = {
                'Content-Type': 'application/json',
                'Authorization': `Bearer ${apiKey}`,
            };
            const body = {
                model: model,
                messages: [
                    {
                        role: 'user',
                        content: prompt,
                    },
                ],
                stream: false,
            };

            sendRequest(url, 'POST', headers, body, proxy).then((result)=>{
                const data = result.data.data;
                const choices = data?.choices;
                if (choices && choices.length > 0 && choices[0].message && choices[0].message.content) {
                    resolve(choices[0].message.content);
                }else{
                    reject('Groq å“åº”æ ¼å¼ä¸æ­£ç¡®æˆ–æ— å†…å®¹');
                }
            }).catch((error)=>{
                // æ¥å£é”™è¯¯ç¤ºä¾‹ï¼š"API è¯·æ±‚å¤±è´¥ï¼ŒçŠ¶æ€ç : 401 Unauthorizedï¼Œå“åº”: {"error":{"message":"Invalid API Key","type":"invalid_request_error","code":"invalid_api_key"}}"
                reject(error);
            });
        })
    };

    /**
     * Google Gemini æ¨¡å‹
     * @param {string} prompt ç”¨æˆ·è¾“å…¥
     * @param {string} model æ¨¡å‹åç§° (å¦‚ "gemini-1.5-flash", "gemini-1.5-pro")
     * @param {string} apiKey API å¯†é’¥
     * @param {string} [proxy] å¯é€‰çš„ä»£ç†åœ°å€
     * @returns {Promise<{success: boolean, data?: string, error?: string}>} AI æ¨¡å‹è¾“å‡ºæˆ–é”™è¯¯ä¿¡æ¯
     */
    const google = (prompt, model, apiKey, proxy) => {
        return new Promise((resolve, reject) => {
            const url = `https://generativelanguage.googleapis.com/v1beta/models/${model}:generateContent?key=${apiKey}`;
            const headers = {
                'Content-Type': 'application/json',
            };
            const body = {
                contents: [
                    {
                        parts: [
                            {
                                text: prompt,
                            },
                        ],
                    },
                ],
            };

            sendRequest(url, 'POST', headers, body, proxy).then((result)=>{
                const data = result.data.data;
                const candidates = data?.candidates;
                if (candidates && candidates.length > 0 && candidates[0].content && candidates[0].content.parts && candidates[0].content.parts.length > 0) {
                    const textPart = candidates[0].content.parts.find(part => part.text);
                    if (textPart) {
                        resolve(textPart.text);
                        return;
                    }
                }

                reject('Google Gemini å“åº”æ ¼å¼ä¸æ­£ç¡®æˆ–æ— å†…å®¹');
            }).catch((error)=>{
                /**
                 * æ¥å£é”™è¯¯ç¤ºä¾‹ï¼š
                 * "API è¯·æ±‚å¤±è´¥ï¼ŒçŠ¶æ€ç : 400 Bad Requestï¼Œå“åº”: {
                    "error": {
                        "code": 400,
                        "message": "API key not valid. Please pass a valid API key.",
                        "status": "INVALID_ARGUMENT",
                        "details": [
                        {
                            "@type": "type.googleapis.com/google.rpc.ErrorInfo",
                            "reason": "API_KEY_INVALID",
                            "domain": "googleapis.com",
                            "metadata": {
                            "service": "generativelanguage.googleapis.com"
                            }
                        },
                        {
                            "@type": "type.googleapis.com/google.rpc.LocalizedMessage",
                            "locale": "en-US",
                            "message": "API key not valid. Please pass a valid API key."
                        }
                        ]
                    }
                    }
                    "
                 */
                reject(error);
            })
        })
    };

    /**
     * ChatGPT æ¨¡å‹
     * @param {string} prompt ç”¨æˆ·è¾“å…¥
     * @param {string} model æ¨¡å‹åç§° (å¦‚ "gpt-4.1", "gpt-3.5-turbo-0125")
     * @param {string} apiKey API å¯†é’¥
     * @param {string} [proxy] å¯é€‰çš„ä»£ç†åœ°å€
     * @returns {Promise<{success: boolean, data?: string, error?: string}>} AI æ¨¡å‹è¾“å‡ºæˆ–é”™è¯¯ä¿¡æ¯
     */
    const chatGPT = (prompt, model, apiKey, proxy) => {
        return new Promise((resolve, reject) => {
            const url = 'https://api.openai.com/v1/chat/completions';
            const headers = {
                'Content-Type': 'application/json',
                'Authorization': `Bearer ${apiKey}`,
            };
            const body = {
                model: model,
                messages: [
                    {
                        role: "user",
                        content: prompt
                    }
                ]
            };

            sendRequest(url, 'POST', headers, body, proxy).then((result)=>{
                const data = result.data.data;
                const choices = data?.choices;
                if (choices && choices.length > 0 && choices[0].message && choices[0].message.content) {
                    resolve(choices[0].message.content);
                }else{
                    reject('ChatGPT å“åº”æ ¼å¼ä¸æ­£ç¡®æˆ–æ— å†…å®¹');
                }
            }).catch((error)=>{
                /**
                 * æ¥å£é”™è¯¯è¿”å›ç¤ºä¾‹ï¼š"API è¯·æ±‚å¤±è´¥ï¼ŒçŠ¶æ€ç : 401 Unauthorizedï¼Œå“åº”: {
                        "error": {
                            "message": "Incorrect API key provided: qqqqqqqq**********1231. You can find your API key at https://platform.openai.com/account/api-keys.",
                            "type": "invalid_request_error",
                            "param": null,
                            "code": "invalid_api_key"
                        }
                    }
                    "
                 */
                reject(error);
            })
        })
    };

    const aiAnnotation = (wordsString)=>{
        return new Promise((resolve, reject) => {
            const prompt = `ä½ æ˜¯ä¸€ä¸ªè¯­è¨€ä¸“å®¶ï¼Œç”¨æˆ·ä¼šæŠŠå•è¯ä»¥æ•°ç»„å½¢å¼å‘ç»™ä½ ï¼Œè¯·ä½ ä»¥ç®€ä½“ä¸­æ–‡çš„å½¢å¼è¿”å›ç»™ç”¨æˆ·ï¼Œå¹¶è§£é‡Šå•è¯è¯æ€§ã€å•è¯è¯»éŸ³ã€å•è¯ä¸­æ–‡é‡Šä¹‰ã€é™„åŠ è¯´æ˜ã€ä»¥åŠå…¶é€‚ç”¨æ€§æ˜¯ä»…é™äºæœ¬è¯¾æ–‡è¾“å‡º0ï¼Œè¯¥è¯­è¨€é€šç”¨è¾“å‡º1ã€‚
ä»¥å¦‚ä¸‹æ ¼å¼è¿”å›ï¼Œæ¯ä¸ªå•è¯ä¹‹å‰ç”¨2ä¸ªæ¢è¡Œéš”å¼€ï¼Œä¸è¦è¾“å‡ºå¤šä½™çš„è¯ï¼š
å•è¯ï¼šå•è¯å†…å®¹
è¯æ€§ï¼šå¯é€‰è¾“å‡º noun,numeral,measure_word,verb,adjective,distinguishing_word,adverb,conjunction,preposition,auxiliary,modal_particle,phrase,sentence_fragment,pronoun,interjection,onomatopoeia,morpheme,other
è¯»éŸ³ï¼š
ä¸­æ–‡é‡Šä¹‰ï¼š
é™„åŠ è¯´æ˜ï¼š
é€‚ç”¨æ€§ï¼šå¯é€‰è¾“å‡º 0,1

å•è¯ï¼š${wordsString}`

            const optionStore = useOptionStore();
            const aiOption = optionStore.getAIOption();
            const nowAiPlatform = aiOption.nowAiPlatform;
            const model = aiOption[nowAiPlatform].model;
            const apiKey = aiOption[nowAiPlatform].apiKey;
            const proxy = aiOption[nowAiPlatform].proxy;

            switch(nowAiPlatform){
                case 'ChatGLM':
                    chatGLM(prompt, model, apiKey, proxy).then((result)=>{
                        resolve(result);
                    }).catch((error)=>{
                        reject(error);
                    });
                    break;
                case 'DeepSeek':
                    deepSeek(prompt, model, apiKey, proxy).then((result)=>{
                        resolve(result);
                    }).catch((error)=>{
                        reject(error);
                    });
                    break;
                case 'Groq':
                    groq(prompt, model, apiKey, proxy).then((result)=>{
                        resolve(result);
                    }).catch((error)=>{
                        reject(error);
                    });
                    break;
                case 'Google':
                    google(prompt, model, apiKey, proxy).then((result)=>{
                        resolve(result);
                    }).catch((error)=>{
                        reject(error);
                    });
                    break;
                case 'ChatGPT':
                    chatGPT(prompt, model, apiKey, proxy).then((result)=>{
                        resolve(result);
                    }).catch((error)=>{
                        reject(error);
                    });
                    break;
                default :
                    reject('æœªçŸ¥çš„AIå¹³å°');
            }
        });
    }

    return {
        chatGLM,
        deepSeek,
        groq,
        google,
        chatGPT,
        aiAnnotation
    };
};

export default useAiChat;